{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0aa5a28",
   "metadata": {},
   "source": [
    "# Neural Network for kids ðŸ§’"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71f59778",
   "metadata": {},
   "source": [
    "## importing library\n",
    "\n",
    "First Let's See how we can use library to train a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "d22df5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36b51e60",
   "metadata": {},
   "source": [
    "## creating basic function\n",
    "\n",
    "Oru problem statement is this  \n",
    "\n",
    "`2*? = 8`\n",
    "\n",
    "I know it's so hard but let's give our computer a hard time,cause why not ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "720af2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return 2*w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f4a44a5",
   "metadata": {},
   "source": [
    "## Let's Also Plot the Function so we can visualize all the output possiblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "263beea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriram\\AppData\\Local\\Temp\\ipykernel_11460\\3816062301.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  n = torch.range(-10,10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2177ab84190>]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = torch.range(-10,10)\n",
    "y = f(n)\n",
    "plt.plot(n,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "404d76fb",
   "metadata": {},
   "source": [
    "As you can See it's a linear function(it doesn't turn,just a stright line) \n",
    "\n",
    "You don't need to know this for now but just giving a headsup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "159e9a7d",
   "metadata": {},
   "source": [
    "## using with library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f291164",
   "metadata": {},
   "source": [
    "Let's Create a respective variables we needed\n",
    "\n",
    "initialize them as \n",
    "\n",
    "W = 2\n",
    "\n",
    "y = 8\n",
    "\n",
    "W = weights -> which is a going to contains the answer for question mark on our problem `2*?=8` \n",
    "\n",
    "but for now it contains some random value like 2\n",
    "\n",
    "y = It's going to be our output Value -> 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "4224aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.0) ; w.requires_grad=True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ce13b73",
   "metadata": {},
   "source": [
    "Now you probably thinking if you never used Torch What is `Requires_grad` means\n",
    "\n",
    "Well this is what going to do the calculus(learning of the neural Network) behind the scene\n",
    "\n",
    "We will see this in detail below just hang in there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "91fca3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af2fb1ec",
   "metadata": {},
   "source": [
    "Let's do the prediction \n",
    "\n",
    "passing the `w = 2` into our function the output should be the predicted value of our model \n",
    "\n",
    "as for now we didn't train our model so it's going to be random\n",
    "\n",
    "in this case it's going to be `4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "db7af5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = f(w)\n",
    "pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebabbdbb",
   "metadata": {},
   "source": [
    "Now let's calculate the distance between our prediction and the desired output \n",
    "\n",
    "`8 - 4` \n",
    "\n",
    "Which gives the output of \n",
    "\n",
    "`4`\n",
    "\n",
    "Now this is our loss value \n",
    "\n",
    "What is loss ??\n",
    "\n",
    "How much our model is wrong about the data (what's the distance between the desired output and model's prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "cfa76e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = y - pred \n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "160a460a",
   "metadata": {},
   "source": [
    "Now let's do the backward pass \n",
    "\n",
    "What is backward pass ?\n",
    "\n",
    "This is where our model could figure out what are the things it needs to change to minimize the loss\n",
    "\n",
    "(in our case it's the W value)\n",
    "\n",
    "If you still don't know what i'm talking about it's ok it's normal trust me just again hang in there ....\n",
    "\n",
    "it's going to get pretty clear...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "44c5d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f22b5454",
   "metadata": {},
   "source": [
    "Now note here it's important \n",
    "\n",
    "You know that our `W` value is `2` \n",
    "\n",
    "but when i get the `gradient` attribute of our `W`\n",
    "\n",
    "we get `-2` \n",
    "\n",
    "What ????\n",
    "\n",
    "what is gradient ??\n",
    "\n",
    "and where did the `-2` came from\n",
    "\n",
    "shhhhhh.......\n",
    "sh...\n",
    "calm down... \n",
    "\n",
    "everything will be cleared.. as soon as we start break it down and build our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "815987cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad#for now note this value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3734588",
   "metadata": {},
   "source": [
    "Now this is something really intresting \n",
    "\n",
    "AS you can see i'm doing lot of stuff's here \n",
    "\n",
    "which in neural network is called `updating the parameters`\n",
    "\n",
    "which as you can see \n",
    "************************************************\n",
    "\n",
    "we are getting the `w value` = 2 (w.data == just getting the data of the `w` as `int` and not as `torch.tensor` type)\n",
    "\n",
    "************************************************\n",
    "\n",
    "and after that we are updating the w value with some sort of formula which consist of \n",
    "\n",
    "`-` minus operator\n",
    "\n",
    "`0.1` some small number(which is also called as learning rate)\n",
    "\n",
    "`w.grad` which is a gradient(magic number for our model) = `-2`\n",
    "\n",
    "and we will discuss this later in this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "7b4d479a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2000, requires_grad=True)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data = w.data - 0.1* w.grad# for now note the w original data 2 changed into 2.2\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "cf1dbe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4000, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = f(w)\n",
    "pred# which indeed changes our prediction value 2*2.2 = 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "dcbef881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4000, requires_grad=True)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data = w.data - 0.1 * w.grad\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "6f5c8893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8000, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = f(w)\n",
    "pred# which indeed changes our prediction value-> 2*2.4 = 4.8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eae6571",
   "metadata": {},
   "source": [
    "and so on , so on ... sooo onnnn \n",
    "\n",
    "so let's do this in for loop and let's see what is going to happen after a while "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "0c41967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just redeclaring the w and y for convience sake\n",
    "w = torch.tensor(2.0);w.requires_grad=True\n",
    "y = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "df5fd5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is  3.5999999046325684\n",
      "loss is  3.1999998092651367\n",
      "loss is  2.799999713897705\n",
      "loss is  2.3999996185302734\n",
      "loss is  1.9999995231628418\n",
      "loss is  1.5999994277954102\n",
      "loss is  1.1999993324279785\n",
      "loss is  0.7999992370605469\n",
      "loss is  0.39999914169311523\n",
      "loss is  -9.5367431640625e-07\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    pred = f(w)\n",
    "    \n",
    "    loss = y-pred\n",
    "    \n",
    "    w.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    w.data -= 0.1 * w.grad\n",
    "    \n",
    "    pred = f(w)\n",
    "    \n",
    "    loss = y-pred\n",
    "    print(f\"loss is  {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5d72ba2",
   "metadata": {},
   "source": [
    "wow!! holy s... ok i'm not going to say the word but do you know what happened now \n",
    "\n",
    "our loss is $10^-7$ (10 to the power -7) move the decimal number to 7 numbers to the right\n",
    "\n",
    "0.00000095367....\n",
    "\n",
    "which as you can see is really low\n",
    "\n",
    "let's see the w value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "52738a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0000)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data # it's the right answer 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "0e728533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0000, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "182d2637",
   "metadata": {},
   "source": [
    "yeahhiiiiiii \n",
    "\n",
    "ðŸ¥³ wow there we go we got the right answer\n",
    "\n",
    "bye........just kidding \n",
    "\n",
    "ðŸ˜‚\n",
    "\n",
    "more than ever now you have more questions \n",
    "\n",
    "I know i've been there \n",
    "\n",
    "so let's answer it one by one \n",
    "\n",
    "by building our own model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b3206",
   "metadata": {},
   "source": [
    "Doing By hand without any libraries\n",
    "\n",
    "step by step and automatic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975cead",
   "metadata": {},
   "source": [
    "## Step By Step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93b6c920",
   "metadata": {},
   "source": [
    "Ha here we can see there is no library just pure python codes and number's \n",
    "\n",
    "agian the same \n",
    "\n",
    "`w` `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "8432b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2\n",
    "y = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "dae13747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=  f(w)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "71098508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = y-pred\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "3c0aa9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_grad = -2# again we are getting close to finding where did this number came from......\n",
    "w_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "0e592c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w -= 0.1*w_grad\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "74326332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = f(w)\n",
    "pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39058689",
   "metadata": {},
   "source": [
    "just making our future self easy to calculate the loss by functionizing it  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "06d103de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,pred):\n",
    "    return y - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "d8c515c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5999999999999996"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = loss(y,pred)\n",
    "l#it's decreasing as it should"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396f170",
   "metadata": {},
   "source": [
    "## automating the process again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "db21ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return 2*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "23841064",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2\n",
    "y = 24\n",
    "h = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "59f44009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is  19.6\n",
      "loss is  18.799999999999997\n",
      "loss is  18.0\n",
      "loss is  17.199999999999996\n",
      "loss is  16.4\n",
      "loss is  15.599999999999996\n",
      "loss is  14.799999999999995\n",
      "loss is  13.999999999999995\n",
      "loss is  13.199999999999994\n",
      "loss is  12.399999999999993\n",
      "loss is  11.599999999999993\n",
      "loss is  10.799999999999992\n",
      "loss is  9.999999999999991\n",
      "loss is  9.19999999999999\n",
      "loss is  8.39999999999999\n",
      "loss is  7.599999999999991\n",
      "loss is  6.799999999999994\n",
      "loss is  5.9999999999999964\n",
      "loss is  5.199999999999999\n",
      "loss is  4.400000000000002\n",
      "loss is  3.600000000000005\n",
      "loss is  2.800000000000008\n",
      "loss is  2.0000000000000107\n",
      "loss is  1.2000000000000135\n",
      "loss is  0.40000000000001634\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    pred = f(w)\n",
    "    w_grad = 2 * -1\n",
    "    w -= h * w_grad\n",
    "    pred = f(w)\n",
    "    l = loss(y,pred)\n",
    "    if i % 2 == 0:\n",
    "        print(f\"loss is  {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "d64f61f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.999999999999991"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "29d316ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.999999999999982"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = f(w)\n",
    "pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "893ad077",
   "metadata": {},
   "source": [
    "Ok we did the same thing without any libraries by just using pure python coding and the output we got is again the correct one\n",
    "\n",
    "24\n",
    "\n",
    "but now at this point you probabily have two question \n",
    "\n",
    "one is \n",
    "\n",
    "`what is this -2 number where did this came from (calculus)`\n",
    "\n",
    "and\n",
    "\n",
    "`how is the w value increasing to the right amount`\n",
    "\n",
    "let's look at the first question with a problem called \n",
    "\n",
    "`finding the gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1c242",
   "metadata": {},
   "source": [
    "## the calculas behind the scene "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7719e22b",
   "metadata": {},
   "source": [
    "what is the gradient \n",
    "\n",
    "well first let me show you where i learn't it when i started this pet project \n",
    "\n",
    "[click me](https://www.youtube.com/watch?v=-ktrtzYVk_I&list=PL3j1ntBPCU_om5O1RBi5-vDRwVTvDwyeV&ab_channel=NancyPi)\n",
    "\n",
    "which is explained by an angel (i mean literaly she is so beautifullðŸ˜™) sorry for perving out \n",
    "\n",
    "but i'm not kidding she will explain every concept so easily and intrestingly enough that you will never forget\n",
    "\n",
    "but if you want the summary of the information\n",
    "\n",
    "`gradient is like the growing factor of a given function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "7d0feeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's break that sentence down\n",
    "\n",
    "#let's take the example of 2*2 our problem statement\n",
    "x = 2\n",
    "w = 2\n",
    "pred = x*w\n",
    "pred\n",
    "# now i have a question for you how much does the pred change if i increase the w  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "0d85797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see it in action\n",
    "x = 2\n",
    "w = 3\n",
    "pred = x*w\n",
    "pred # whoa it's changed two time the previous value don't trust me "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "ea0d337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the proff\n",
    "6-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "03f2ea40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again changing the w \n",
    "x = 2\n",
    "w = 4\n",
    "pred = x*w\n",
    "pred# again the same two times the previous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "6f805eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the proff\n",
    "8-6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eb054c4",
   "metadata": {},
   "source": [
    "### but what if i changed the x value a little bit and increased the w value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "90f9c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 9 12\n"
     ]
    }
   ],
   "source": [
    "x = 3#changed the x => 2 -> 3\n",
    "w = 2# is same\n",
    "pred_1 = x*w\n",
    "\n",
    "w = 3#changed from w=> 2 -> 3 \n",
    "pred_2 = x*w\n",
    "\n",
    "w = 4# changed again\n",
    "pred_3 = x*w\n",
    "\n",
    "#printing the output\n",
    "print(pred_1,pred_2,pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d4e65bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the difference \n",
    "9-6,12-9# no wonder it's rate of change is 3 times "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96171b34",
   "metadata": {},
   "source": [
    "So at this point you got the idea that \n",
    "\n",
    "if i changed `w` it's going to increase the prediction value in a rate of `x` times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d511e5b9",
   "metadata": {},
   "source": [
    "## And Gentlemens and ladies this is what called gradient value\n",
    "\n",
    "which is saying that what is a amount of change occur in a prediction if i change the `w` \n",
    "\n",
    "the answer is `x`\n",
    "\n",
    "that's the amount of change going to happen\n",
    "\n",
    "**that's the derivative of the multiplication function**\n",
    "\n",
    "and the gradient is the `x` value\n",
    "\n",
    "at first problem we had `x` value has `2`\n",
    "\n",
    "so there our `w` gradient value is `2`\n",
    "\n",
    "and now we changed our `x` value into `3`\n",
    "\n",
    "now the gradient value for `w` is `3`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79ae7841",
   "metadata": {},
   "source": [
    "### so now we know what is the derivative of our multiplicative function is \n",
    "\n",
    "pred = x*w\n",
    "\n",
    "with respective w (if i change w in a slight way how much it's going to change the pred value)\n",
    "\n",
    "answer is :-> x (amount)\n",
    "\n",
    "and yes viseversa\n",
    "\n",
    "pred = x*w\n",
    "\n",
    "with respective x (if i change x in a slight way how much it's going to change the pred value)\n",
    "\n",
    "answer is :-> w (amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "d3961589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's call our derivative of the w (dw)\n",
    "x = 2\n",
    "w = 2\n",
    "y = 8 \n",
    "dw = x\n",
    "dw#the gradient value is 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "085bb2fe",
   "metadata": {},
   "source": [
    "### ok until now we got the gradient value has 2 \n",
    "\n",
    "but if you can recall we had a -2 has a gradient before\n",
    "\n",
    "so let's find our where we got the `-` symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "0c99c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction value before changing the w: 4\n",
      "the loss value before changing w: 4\n",
      "the prediction value after changing the w: 6\n",
      "the loss value after changing the w : 2\n"
     ]
    }
   ],
   "source": [
    "#let's check how much does the loss change with repective w\n",
    "\n",
    "x = 2\n",
    "w = 2\n",
    "dw = x\n",
    "y = 8\n",
    "pred = x*w\n",
    "print(f\"the prediction value before changing the w: {pred}\")\n",
    "print(f\"the loss value before changing w: {loss(y,pred)}\")\n",
    "\n",
    "\n",
    "w = 3 \n",
    "pred = x*w\n",
    "print(f\"the prediction value after changing the w: {pred}\")\n",
    "print(f\"the loss value after changing the w : {loss(y,pred)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0bf2e34",
   "metadata": {},
   "source": [
    "we know why we got the prediction->6 \n",
    "\n",
    "but what about loss->2\n",
    "\n",
    "so turn's out that our loss contain the formula of \n",
    "\n",
    "`y-pred` -> (8-4)\n",
    "\n",
    "which has minus value \n",
    "\n",
    "so what ever value your going to give to the pred it's value is going to change in - value because again we are reducing from 8\n",
    "\n",
    "so with respective to minus operator \n",
    "\n",
    "we will get -1 \n",
    "\n",
    "and for plus operator we get the opposite as you may already guessed it \n",
    "\n",
    "+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "d83aea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 2\n",
    "y = 8\n",
    "l = y - w\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "13ed5e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 3 \n",
    "y = 8\n",
    "l = y-w\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "8624f6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 4#it's dw value is 0 (because it's just a constant it's not going to change with it's input)\n",
    "y = 8\n",
    "l = y-w\n",
    "l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa50b96f",
   "metadata": {},
   "source": [
    "You get the idea \n",
    "\n",
    "but we also have a x (you can see in the above example we didn't take the prediction formula x*w)\n",
    "\n",
    "we know that our w has a dw which is 2 \n",
    "\n",
    "so now we use chain rule which is (chain rule = multiplying the current gradient value to the previous gradient)\n",
    "\n",
    "dl*dw (here dl = l derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "d94725f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now with repective w \n",
    "dl = -1\n",
    "dw = 2 * dl\n",
    "dw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc62c39",
   "metadata": {},
   "source": [
    "yeahhhhhh .... ðŸ¥³\n",
    "and that's the gradient and calculas part of the neural network\n",
    "\n",
    "the fundamentals are not gonna change\n",
    "\n",
    "and this is the fundamentals ðŸ¥³\n",
    "\n",
    "if you want to understand it with more example i highly recommed you watch the angel(ðŸ¥°ðŸ˜‡) video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "802ad8a6",
   "metadata": {},
   "source": [
    "In summary\n",
    "\n",
    "`loss = y - pred` \n",
    "\n",
    "dl/dl = -1\n",
    "\n",
    "`pred = x*w`\n",
    "\n",
    "dl/dx = w\n",
    "\n",
    "dl/dw = x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06e4c08f",
   "metadata": {},
   "source": [
    "### now you have second question \n",
    "\n",
    "updation \n",
    "\n",
    "w = 2\n",
    "\n",
    "x = 2 \n",
    "\n",
    "dw = x\n",
    "\n",
    "`w = w - 0.1 * dw`\n",
    "\n",
    "**************************************\n",
    "\n",
    "For this you need to understand what does the gradient value tell's us \n",
    "\n",
    "let's take this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "98061c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 2\n",
    "x = 2\n",
    "pred = x*w\n",
    "l = loss(y,pred)\n",
    "dl = -1\n",
    "dw = x * dl\n",
    "l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8b45d84",
   "metadata": {},
   "source": [
    "#### now our gradient value dw=-2 is telling us if we decrease the w value by 2 \n",
    "\n",
    "we increase the loss (but we don't want to increase the loss we decrease the loss for getting the correct answer)\n",
    "\n",
    "so  we increase the w value by 2 \n",
    "\n",
    "(generalizing the formula would be \n",
    "\n",
    "`w = w - (-w_grad)`\n",
    " \n",
    " which would increase and and if we got plus gradient value \n",
    " \n",
    " `w = w - (w_grad)` \n",
    " \n",
    " it would decrease the w as we want to) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "f676750a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = w - dw # note here we didn't use the small value 0.1\n",
    "w "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94de0070",
   "metadata": {},
   "source": [
    "Great here we go the 4 with just one step of running the code (we didn't use the for loop like before)\n",
    "\n",
    "so why do we need to use the small value if we can do this in this way easily \n",
    "\n",
    "for that let's take the another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "f4ea15f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 6\n"
     ]
    }
   ],
   "source": [
    "x = 2 \n",
    "w = 2\n",
    "y = 10\n",
    "\n",
    "pred = f(w)\n",
    "l = loss(y,pred)\n",
    "\n",
    "dl = -1\n",
    "dw = x * dl\n",
    "\n",
    "print(f\"loss is : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "41049257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 2\n"
     ]
    }
   ],
   "source": [
    "w = w - dw\n",
    "\n",
    "pred = f(w)\n",
    "l = loss(y,pred)\n",
    "\n",
    "print(f\"loss is : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "d7348b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : -2\n"
     ]
    }
   ],
   "source": [
    "w = w - dw\n",
    "\n",
    "pred = f(w)\n",
    "l = loss(y,pred)\n",
    "\n",
    "print(f\"loss is : {l}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dca58f89",
   "metadata": {},
   "source": [
    "Here in this example as you can see it's overshooting the loss \n",
    "\n",
    "which means\n",
    "\n",
    "the w value is going to go larger than what we want it to be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "f7f57d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w # but we know that 2 * 5  = 10 and not 2 * 6 = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebcb7313",
   "metadata": {},
   "source": [
    "so if we decrease the the w value with small amount of gradient at a step we would converge to the loss smoothly and preciesly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "050dd48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 6\n"
     ]
    }
   ],
   "source": [
    "x = 2 \n",
    "w = 2\n",
    "y = 10\n",
    "\n",
    "pred = f(w)\n",
    "l = loss(y,pred)\n",
    "\n",
    "dl = -1\n",
    "dw = x * dl\n",
    "\n",
    "print(f\"loss is : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "685bc430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 5.6\n"
     ]
    }
   ],
   "source": [
    "w = w - 0.1*dw\n",
    "\n",
    "pred = f(w)\n",
    "l = loss(y,pred)\n",
    "\n",
    "print(f\"loss is : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "73d9109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "w = w - 0.1*dw\n",
    "\n",
    "pred = f(w)\n",
    "l = loss(y,pred)\n",
    "\n",
    "print(f\"loss is : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "3cadad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 4.799999999999999\n"
     ]
    }
   ],
   "source": [
    "w = w - 0.1*dw\n",
    "\n",
    "pred = f(w)\n",
    "l = loss(y,pred)\n",
    "\n",
    "print(f\"loss is : {l}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebf6fb5d",
   "metadata": {},
   "source": [
    "You get the idea let's for loop the shit out of this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "a2453c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is : 4.399999999999999\n",
      "loss is : 3.9999999999999982\n",
      "loss is : 3.599999999999998\n",
      "loss is : 3.1999999999999975\n",
      "loss is : 2.799999999999997\n",
      "loss is : 2.399999999999997\n",
      "loss is : 1.9999999999999964\n",
      "loss is : 1.599999999999996\n",
      "loss is : 1.1999999999999957\n",
      "loss is : 0.7999999999999954\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):# this 10 is also called epochs value (eg: epochs value for this model is 10)\n",
    "    w = w - 0.1*dw\n",
    "    pred = f(w)\n",
    "    l = loss(y,pred)\n",
    "\n",
    "    print(f\"loss is : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "4cd2e003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.600000000000002, 5)"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,round(w)#normal w value and rounded of w value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63b03abe",
   "metadata": {},
   "source": [
    "Yes it would take lot of time and iteration to converge but heyyyy it's atleast precise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ca7acd2",
   "metadata": {},
   "source": [
    "Now You aldready understand the basics of Neural Network\n",
    "\n",
    "But let's sum this up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fe314e2",
   "metadata": {},
   "source": [
    "***************************************\n",
    "W = 2 # which is called weight's (our model is going to change this)\n",
    "\n",
    "x = 2 #our original value (we can't change this)\n",
    "\n",
    "y = 8 # our Desired output\n",
    "\n",
    "lr = 0.1 # to slow down the process of updating to make it precise (learning rate)\n",
    "\n",
    "pred = x * w # simple linear function \n",
    "\n",
    "loss = y - pred # our loss (distance between the desired output and prediction)\n",
    "\n",
    "dl = -1 # the gradient for our loss function\n",
    "\n",
    "dw = x * dl # the gradient for our Weights with chain rule \n",
    "***********************************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f641b99",
   "metadata": {},
   "source": [
    "## Fully put together with full Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "db568117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    return x*w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da5890cc",
   "metadata": {},
   "source": [
    "### change the y value as greater or as much as you want our AI model is going to predict the right W value for that (keep the y value with a divisible of 2 to get pretty accurate answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "2f6d111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n",
      "loss is :  318.4\n",
      "loss is :  314.4\n",
      "loss is :  310.4\n",
      "loss is :  306.4\n",
      "loss is :  302.40000000000003\n",
      "loss is :  298.40000000000003\n",
      "loss is :  294.40000000000003\n",
      "loss is :  290.40000000000003\n",
      "loss is :  286.4000000000001\n",
      "loss is :  282.4000000000001\n",
      "loss is :  278.4000000000001\n",
      "loss is :  274.4000000000001\n",
      "loss is :  270.40000000000015\n",
      "loss is :  266.40000000000015\n",
      "loss is :  262.40000000000015\n",
      "loss is :  258.40000000000015\n",
      "loss is :  254.4000000000001\n",
      "loss is :  250.40000000000003\n",
      "loss is :  246.39999999999998\n",
      "loss is :  242.39999999999992\n",
      "loss is :  238.39999999999986\n",
      "loss is :  234.3999999999998\n",
      "loss is :  230.39999999999975\n",
      "loss is :  226.3999999999997\n",
      "loss is :  222.39999999999964\n",
      "loss is :  218.39999999999958\n",
      "loss is :  214.39999999999952\n",
      "loss is :  210.39999999999947\n",
      "loss is :  206.3999999999994\n",
      "loss is :  202.39999999999935\n",
      "loss is :  198.3999999999993\n",
      "loss is :  194.39999999999924\n",
      "loss is :  190.39999999999918\n",
      "loss is :  186.39999999999912\n",
      "loss is :  182.39999999999907\n",
      "loss is :  178.399999999999\n",
      "loss is :  174.39999999999895\n",
      "loss is :  170.3999999999989\n",
      "loss is :  166.39999999999884\n",
      "loss is :  162.39999999999878\n",
      "loss is :  158.39999999999873\n",
      "loss is :  154.39999999999867\n",
      "loss is :  150.3999999999986\n",
      "loss is :  146.39999999999856\n",
      "loss is :  142.3999999999985\n",
      "loss is :  138.39999999999844\n",
      "loss is :  134.39999999999839\n",
      "loss is :  130.39999999999833\n",
      "loss is :  126.39999999999827\n",
      "loss is :  122.39999999999822\n",
      "loss is :  118.39999999999816\n",
      "loss is :  114.3999999999981\n",
      "loss is :  110.39999999999804\n",
      "loss is :  106.39999999999799\n",
      "loss is :  102.39999999999793\n",
      "loss is :  98.39999999999787\n",
      "loss is :  94.39999999999782\n",
      "loss is :  90.39999999999776\n",
      "loss is :  86.3999999999977\n",
      "loss is :  82.39999999999765\n",
      "loss is :  78.39999999999759\n",
      "loss is :  74.39999999999753\n",
      "loss is :  70.39999999999748\n",
      "loss is :  66.39999999999753\n",
      "loss is :  62.39999999999776\n",
      "loss is :  58.39999999999799\n",
      "loss is :  54.399999999998215\n",
      "loss is :  50.39999999999844\n",
      "loss is :  46.39999999999867\n",
      "loss is :  42.3999999999989\n",
      "loss is :  38.399999999999125\n",
      "loss is :  34.39999999999935\n",
      "loss is :  30.39999999999958\n",
      "loss is :  26.399999999999807\n",
      "loss is :  22.400000000000034\n",
      "loss is :  18.40000000000026\n",
      "loss is :  14.400000000000489\n",
      "loss is :  10.400000000000716\n",
      "loss is :  6.400000000000944\n",
      "loss is :  2.400000000001171\n"
     ]
    }
   ],
   "source": [
    "w = 1\n",
    "x = 2\n",
    "x = 2\n",
    "y = 324 # change the output if you want in a positive direction to any number you want\n",
    "lr = 0.1\n",
    "i = 0\n",
    "l = loss(y,f(w))\n",
    "print(l)\n",
    "while l>0.1:\n",
    "    \n",
    "    i+=1\n",
    "    #forward pass\n",
    "    pred = f(w)\n",
    "    \n",
    "    l = loss(y,pred)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"loss is : \",l)\n",
    "        \n",
    "    #backward pass \n",
    "    w_grad = 2 * -1\n",
    "    \n",
    "    #updating the w value\n",
    "    w -= lr * w_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "848050bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.19999999999933"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "b5fa20a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(f(w))# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65148207",
   "metadata": {},
   "source": [
    "That's it you Now fully know What is happening inside the neural network while it's training \n",
    "\n",
    "Now if you want to challenge yourself to understand even further just look down bellow \n",
    "\n",
    "Warning : If you didn't understand the above part clearly enough (I highly recommend Watching my beloved Angel Tutorial and then come back after you understand the above part >>>*all the best*<<<)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a81e6237",
   "metadata": {},
   "source": [
    "## Big one (with real problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8effe5f7",
   "metadata": {},
   "source": [
    "W :-> just a random values for w but this time it contain 3 rows and 3 columns number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "e29009a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2151,  1.0744, -1.3856],\n",
       "        [ 1.2338,  0.3470, -0.5861],\n",
       "        [ 0.8782, -0.6359,  0.9050]], requires_grad=True)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn((3,3),requires_grad=True)\n",
    "w# again it's random numbers in normal distribution(bell curve distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "3d79d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2,4,6],dtype=torch.float32)\n",
    "y = torch.tensor([4,8,12],requires_grad=True,dtype=torch.float32)\n",
    "def f(w,x):\n",
    "    return x@w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79dea634",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e465fd5b",
   "metadata": {},
   "source": [
    "Here as you can see our loss function is some what complecated but not really if you break it down \n",
    "\n",
    "it's `mean squared error` \n",
    "\n",
    "which means \n",
    "\n",
    "$adding((y - pred)^2)/len(y)$\n",
    "\n",
    "squaring the distance so we don't get negative value as a loss \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "391dcba2",
   "metadata": {},
   "source": [
    "****************************\n",
    "So let's break it down \n",
    "\n",
    "1. orginal = y -> y_shape->3\n",
    "\n",
    "2. prediction = x@w -> matrix multiplication x_shape->[3],w_shape->[3,3]\n",
    "\n",
    "3. diff = orginal - prediction\n",
    "\n",
    "4. diff_2 = diff ** 2 -> squaring (just like in the formula)\n",
    "\n",
    "5. sum_dif = suming over(diff_2) again adding the squared differences\n",
    "\n",
    "6. l = len(orginal)**-1 -> which is also represented as ->`1/len(orginal)`\n",
    "\n",
    "7. f_l = l * sum_diff -> which is also represented as `sum_diff/len(orginal)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "10e921cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(79.4787, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loss\n",
    "org = y\n",
    "pred = f(w,x)\n",
    "diff = org - pred\n",
    "diff_2 = diff**2\n",
    "sum_dif = sum(diff_2) \n",
    "l = torch.tensor(len(org)**-1,requires_grad=True)\n",
    "f_l = l*sum_dif\n",
    "f_l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50e8c9b5",
   "metadata": {},
   "source": [
    "Why do we have to break down the formula like this \n",
    "\n",
    "because it's easy to do the `gradient finding` step when we break it down like this compared to the whole formula "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32639164",
   "metadata": {},
   "source": [
    "### We are telling the pytorch to keep the gradient so we can access it globaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "bba7145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.4787, grad_fn=<MulBackward0>)\n",
      "tensor(0.3333, requires_grad=True)\n",
      "tensor(238.4360, grad_fn=<AddBackward0>)\n",
      "tensor([ 33.3425,  68.5309, 136.5625], grad_fn=<PowBackward0>)\n",
      "tensor([-5.7743,  8.2783, 11.6860], grad_fn=<SubBackward0>)\n",
      "tensor([ 4.,  8., 12.], requires_grad=True)\n",
      "tensor([ 9.7743, -0.2783,  0.3140], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "p = [f_l,l,sum_dif,diff_2,diff,org,pred]\n",
    "for i in p:\n",
    "    i.retain_grad()\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7034ba28",
   "metadata": {},
   "source": [
    "We are doing this step to make the torch library to also calculate the gradient and store it in the grad variable so we can check with our own finding and the library finding and see if it both match"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00160f8d",
   "metadata": {},
   "source": [
    "### functionizing our loss (mean squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "7381c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,pred):\n",
    "    org = y.clone().detach()\n",
    "    diff = org - pred\n",
    "    diff_2 = diff**2\n",
    "    sum_dif = sum(diff_2) \n",
    "    l = torch.tensor(len(org)**-1,requires_grad=True)\n",
    "    f_l = l*sum_dif\n",
    "    return f_l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d688f6be",
   "metadata": {},
   "source": [
    "### Calling backward on the full_loss(f_l) to get the gradient for each value's in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "a5e00e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "beea4209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7.6991, -11.0378, -15.5813],\n",
       "        [ 15.3981, -22.0756, -31.1627],\n",
       "        [ 23.0972, -33.1134, -46.7440]])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to have this so we can check whether we got the right gradient by using our manual backpropagation\n",
    "w.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83af650d",
   "metadata": {},
   "source": [
    "### finding the Gradients manually now"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62c3a53d",
   "metadata": {},
   "source": [
    "If you know the power rule which i didn't cover it in this examples...\n",
    "\n",
    "then you can understand the whole thing without any problem \n",
    "\n",
    "in brief \n",
    "\n",
    "if we have the power \n",
    "\n",
    "$2^3$\n",
    "\n",
    "the gradient value is \n",
    "\n",
    "$3*2^2$\n",
    "\n",
    "and another example \n",
    "\n",
    "$2^4$ -> $4*2^3$\n",
    "\n",
    "in general \n",
    "\n",
    "$x^n$ -> $n*x^(n-1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54e4aa85",
   "metadata": {},
   "source": [
    "While finding the gradient like this we have to make sure that shape match with the actual value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "aa25cd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(79.4787, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_l.shape# it has zero dimention(shape) which means it contains only one value\n",
    "f_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "77abf493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333, requires_grad=True)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape# same it has zero dimention\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "40117afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(238.4360, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_dif.shape# same\n",
    "sum_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "d8a6c20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 33.3425,  68.5309, 136.5625], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_2.shape# here we can see the shape/size is 3\n",
    "diff_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "97eacfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.shape # this shape is also three values\n",
    "# diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "e65e7ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape#same here 3 values\n",
    "w.shape# and this value contain 2 dimention (rows,columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd3a15fe",
   "metadata": {},
   "source": [
    "so the matching gradient is also supposed to have that much gradient values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "66d4ea1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(79.4787, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#going from f_l -> w to find the final W_grad \n",
    "org = y\n",
    "pred = x @ w\n",
    "diff = org - pred\n",
    "diff_2 = diff**2\n",
    "sum_dif = sum(diff_2) \n",
    "l = torch.tensor(len(org)**-1,requires_grad=True)\n",
    "f_l = l*sum_dif\n",
    "f_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "fe19cdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7.6991, -11.0378, -15.5813],\n",
       "        [ 15.3981, -22.0756, -31.1627],\n",
       "        [ 23.0972, -33.1134, -46.7440]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#starting from the start f_l\n",
    "f_l_grad = 1\n",
    "l_grad = sum_dif * f_l_grad\n",
    "sum_dif_grad = l * f_l_grad\n",
    "diff_2_grad = torch.ones(diff_2.shape) * sum_dif_grad\n",
    "diff_grad = (2*diff)* diff_2_grad\n",
    "pred_grad = -1* diff_grad\n",
    "#ended with w (also used chain rule as you can see here)\n",
    "w_grad = (torch.ones(w.shape)*x).T *pred_grad\n",
    "w_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "e1e75852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_grad.shape#just like l it has no size which means it has no dimention only one value\n",
    "# l_grad # just uncomment this to check it out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "fe6a93bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_grad.shape#just like the pred_grad it has 3 shape/size \n",
    "# pred_grad # just uncomment this to check it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "826e0a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_grad.shape #just like w it has shape of 3,3 which is (rows,columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "021fe692",
   "metadata": {},
   "source": [
    "### intializing the Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "3fcdbb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn((3,3),requires_grad=True)\n",
    "x = torch.tensor([2,4,6],dtype=torch.float32)\n",
    "y = torch.tensor([8,10,12])\n",
    "def f(w,x):\n",
    "    return x@w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c57ae777",
   "metadata": {},
   "source": [
    "### Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "53ebb65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 191.09176635742188\n",
      "loss : 90.44029998779297\n",
      "loss : 42.80375289916992\n",
      "loss : 20.25824737548828\n",
      "loss : 9.587871551513672\n",
      "loss : 4.537772178649902\n",
      "loss : 2.1476447582244873\n",
      "loss : 1.0164430141448975\n",
      "loss : 0.48106202483177185\n"
     ]
    }
   ],
   "source": [
    "for i in range(900):\n",
    "    #forward pass\n",
    "    pred = f(w,x)\n",
    "\n",
    "    #loss function mse\n",
    "    org = y.clone().detach()\n",
    "    pred = f(w,x)\n",
    "    diff = org - pred\n",
    "    diff_2 = diff**2\n",
    "    sum_dif = sum(diff_2) \n",
    "    l = torch.tensor(len(org)**-1,requires_grad=True)\n",
    "    f_l = l*sum_dif\n",
    "\n",
    "    #backward pass ----\n",
    "    \n",
    "    #backward/chain rule\n",
    "    f_l_grad = 1\n",
    "    l_grad = sum_dif * f_l_grad\n",
    "    sum_dif_grad = l * f_l_grad\n",
    "    diff_2_grad = torch.ones(diff_2.shape) * sum_dif_grad\n",
    "    diff_grad = (2*diff)* diff_2_grad\n",
    "    pred_grad = -1* diff_grad\n",
    "    w_grad = (torch.ones(w.shape)*x).T *pred_grad\n",
    "    \n",
    "    ##updating\n",
    "    lr = 0.001 if l >=6.0 else 0.0001\n",
    "    w.data -= lr * w_grad\n",
    "    #-----------\n",
    "    \n",
    "    #calculating the loss\n",
    "    pred = f(w,x)\n",
    "    l = loss(y,pred)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"loss : {l}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cc0ae4d",
   "metadata": {},
   "source": [
    "### Checking the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "6a91a7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7.4748,  9.6214, 11.4814], grad_fn=<SqueezeBackward3>),\n",
       " tensor([ 8, 10, 12]))"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(w,x),y # agian it's not precise but it's way close if you round every value in f(w,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "b8f1928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7., 10., 11.])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = f(w,x)\n",
    "torch.round(p.data)# as i said it's founded by the computer(not by us ðŸ¤¯)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afd39397",
   "metadata": {},
   "source": [
    "### Checking the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "30433cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0233, -0.4248,  0.1876],\n",
       "        [-0.1255,  0.4546,  0.6322],\n",
       "        [ 0.9884,  1.4421,  1.4296]], requires_grad=True)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w # it's some patterns that we don't have to care about (it's our model learn't pattern for our data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44ceb12b",
   "metadata": {},
   "source": [
    "## Let's do this with Torch (so we can appericate the library even better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "ec95cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn((3,3),requires_grad=True)\n",
    "x = torch.tensor([2,4,6],dtype=torch.float32)\n",
    "y = torch.tensor([4,8,12])\n",
    "def f(w,x):\n",
    "    return x@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "90db3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is : 73.54881286621094\n",
      "the loss is : 62.778648376464844\n",
      "the loss is : 48.646888732910156\n",
      "the loss is : 33.244178771972656\n",
      "the loss is : 18.84918212890625\n",
      "the loss is : 7.5915045738220215\n",
      "the loss is : 1.1365878582000732\n",
      "the loss is : 0.43937358260154724\n"
     ]
    }
   ],
   "source": [
    "for _ in range(8):\n",
    "    \n",
    "    #forward pass\n",
    "    pred =f(w,x)\n",
    "    l = loss(y,pred)\n",
    "\n",
    "    #gradient\n",
    "    l.backward()#that's it ðŸ˜‚\n",
    "\n",
    "    #updating\n",
    "    if l >= 0.6:\n",
    "        w.data -= 0.001 * w.grad\n",
    "    else:\n",
    "        w.data -= 0.00001 * w.grad\n",
    "\n",
    "    #calculating loss\n",
    "    pred =f(w,x)\n",
    "    l = loss(y,pred)\n",
    "    print(f\"the loss is : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "9c5bbd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.5225,  8.3340, 12.9662], grad_fn=<SqueezeBackward3>),\n",
       " tensor([ 4,  8, 12]))"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(w,x),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "7ad3102b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0398,  0.0924,  0.4084],\n",
       "        [-0.5070,  0.5175,  1.0363],\n",
       "        [ 1.4383,  1.0132,  1.3340]], requires_grad=True)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w# unlike the w we used in the above example which has one value 4 here it has 3,3 -> 6 values \n",
    "#to represent that 3 outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48b64dd5",
   "metadata": {},
   "source": [
    "## Transfer Learning Basic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfe66057",
   "metadata": {},
   "source": [
    "Now if we saved this w in a text document or like a file format\n",
    "\n",
    "So if we opened our model next time we could just load this weights saved file and use that weight's to show the model output\n",
    "\n",
    "without Even have to Train the model from scratch \n",
    "\n",
    "This is called Transfer Learning (Pretrained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "c9eaf93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5225,  8.3340, 12.9662], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@w # transfer learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61f3f3d2",
   "metadata": {},
   "source": [
    "Now you can confidently say that you created AI from scratch which can learn to predict the correct multiplicative term for the given outputðŸ¥³ðŸ¥³ðŸ±â€ðŸ’»ðŸ±â€ðŸ‰ðŸ±â€ðŸ‰"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdf0f3f0",
   "metadata": {},
   "source": [
    "## Thankyou For your Time ðŸ’\n",
    "\n",
    "I REALLY APPRECIATE IT ðŸ˜Š\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "db29d9c3531fbefb8528bf1617aed628388b45e6ce014ed5e5e3db8968ae6306"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
